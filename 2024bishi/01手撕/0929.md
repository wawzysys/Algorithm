# LightGBM模型相比与XGBoost的优势在哪里
1速度和效率：
直方图算法：LightGBM使用基于直方图的方法，将连续特征值离散化，从而减少了内存使用和计算时间。这使得它在处理大规模数据时，训练速度明显快于XGBoost。
更少的内存占用：由于直方图的使用，LightGBM能够在更低的内存消耗下处理更多的数据。
2支持更大规模的数据集：
LightGBM能够处理更大规模的数据集和特征，适合高维度数据，尤其在特征数量非常多的情况下，其性能更为突出。
3高效的并行计算：
LightGBM实现了基于数据并行和特征并行的高效并行计算，使其能够更好地利用多核CPU，提升训练速度。


# L1正则化和L2正则化
L1正则化的目标是在模型的损失函数中加入权重参数的绝对值和（即L1范数）的惩罚项。L1正则化会导致一些权重系数被缩小为零。由于L1正则化倾向于选择部分特征而忽略其他特征，因此具有稀疏性，它常用于特征选择中（即可以减少特征的数量）。
L1正则化的稀疏性使得它适合于特征非常多但有很多不相关特征的数据集。
L2正则化不会使权重变为零，而是会使所有权重趋向较小的数值。它倾向于“平滑”权重，而不是完全去除某些特征。

# Informer（2021）
Informer是专门设计用于长时序预测的模型，提出了一些关键的改进来应对传统时序预测模型（如Transformer）在长时间序列中的局限性
ProbSparse Self-Attention: Informer提出了一种稀疏自注意力机制（ProbSparse Attention），用于缓解常规Transformer的全局自注意力在长序列处理上的计算开销。通过优先计算与输入序列中重要位置的注意力，Informer降低了时间和空间复杂度。
长范围依赖捕捉: 基于稀疏注意力机制，Informer可以有效捕获长时间序列中的全局依赖性，适合于长时间序列的预测任务。
多尺度建模: Informer通过在不同时间尺度上建模输入序列，进一步提升了对长时间序列的捕捉能力。
高效性: 相比于传统Transformer，Informer极大减少了计算和存储成本，复杂度由原始的O(L^2))降为O(LlogL)，其中L是输入序列长度
2，FEDformer（2022）
特点：
混合的变换模块（Hybrid Transformer Modules）: FEDformer通过将傅里叶变换（Fourier Transform）和波动变换（Wavelet Transform）相结合，将时间序列分解为不同频率成分，以便更好地进行建模。傅里叶变换用于捕捉时间序列的周期性信息，波动变换则捕捉局部时序变化。
时序分解与整合: FEDformer通过频域中的时间序列分解来捕捉长时间和短时间依赖性，并利用自适应的机制来平衡不同频率成分的贡献。
高效性与稀疏性: FEDformer也应用了稀疏的注意力机制，并在傅里叶域中进行操作，从而进一步减少了计算开销，同时保证了在长时序预测中的准确性。
灵活性: FEDformer不仅可以处理周期性数据，还能够有效应对带有局部变化或噪声的时间序列
TimesNet（2023）
时间块（Time-Block）设计: TimesNet引入了时间块的概念，通过构造不同时间尺度上的子网络（sub-network），来捕捉时间序列的局部和全局模式。每个时间块专注于处理特定的时间尺度，使得模型可以适应不同的时间范围。
空间-时间特征捕捉: TimesNet不仅能够处理时间维度的特征，还可以捕捉时间序列中各个变量之间的相互关系（即空间特征）。这种空间-时间联合建模的方式使其能够更好地处理多维时序数据。
模块化设计: TimesNet的结构是高度模块化的，允许通过堆叠不同的时间块来适应多种时间序列的模式。这种模块化设计使得模型具有很强的灵活性和可扩展性。
高效训练与推理: 与传统Transformer或类似模型相比，TimesNet通过模块化结构和分层建模，有效减少了参数量，并且在长序列预测中的训练和推理过程更加高效。
总结
Informer: 通过ProbSparse自注意力机制和多尺度建模，专注于高效处理长时间序列，减少了计算复杂度。
FEDformer: 引入傅里叶和波动变换来捕捉不同频率的时间序列模式，进一步优化了长时序预测的精度和效率，尤其适用于具有周期性和局部变化的数据。
TimesNet: 针对多维时间序列的复杂模式，通过时间块和空间-时间联合建模，提供了一种模块化、灵活的解决方案，特别适用于复杂多维数据的预测任务。

# 